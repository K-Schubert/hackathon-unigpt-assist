{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d786f2c-7d5c-4fd5-b767-b7626a6c5838",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56ce62-9ec0-4108-bb26-75814c8dc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "import uuid\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "\n",
    "from rag import init_pinecone, init_vectorstore, init_retriever, init_retrievalqa_chain\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384831c7-a2b7-4a0a-9ba8-de304e436d9d",
   "metadata": {},
   "source": [
    "# Import env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7867ba0c-22b7-43fe-b38b-a8174a667934",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT')\n",
    "PINECONE_INDEX= os.getenv('PINECONE_INDEX')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a49e79-630e-4268-8f15-0ccf48bc666f",
   "metadata": {},
   "source": [
    "# Init RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c44b1d-f91d-45b2-87c5-9f0d4c64f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pinecone():\n",
    "\n",
    "    # init pinecone index\n",
    "    pinecone.init(\n",
    "        api_key=PINECONE_API_KEY,\n",
    "        environment=PINECONE_ENVIRONMENT,\n",
    "    )\n",
    "\n",
    "    if PINECONE_INDEX not in pinecone.list_indexes():\n",
    "        pinecone.create_index(PINECONE_INDEX, dimension=1536, metric=\"cosine\")\n",
    "\n",
    "    index = pinecone.Index(PINECONE_INDEX)\n",
    "\n",
    "    return index\n",
    "\n",
    "def init_vectorstore():\n",
    "\n",
    "    # init embedding function\n",
    "    embedding_function = OpenAIEmbeddings(model=\"text-embedding-ada-002\",\n",
    "                                disallowed_special=())\n",
    "\n",
    "    vectorstore = Pinecone.from_existing_index(\n",
    "                index_name=PINECONE_INDEX,\n",
    "                embedding=embedding_function,\n",
    "                #namespace=namespace\n",
    "                )\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "def init_retriever(k):\n",
    "\n",
    "    # init vectorstore\n",
    "    vectorstore = init_vectorstore()\n",
    "\n",
    "    # init retriever\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k}, return_source_documents=True)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "def init_retrievalqa_chain():\n",
    "\n",
    "    index = init_pinecone()\n",
    "    retriever = init_retriever(k=5)\n",
    "\n",
    "    # init llm\n",
    "    llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n",
    "                    model=\"gpt-4-1106-preview\",\n",
    "                    temperature=0,\n",
    "                    streaming=True,\n",
    "                    callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "    # init prompt template\n",
    "    template = \"\"\"\n",
    "        Vous êtes un assistant qui répond à des questions sur l'Université de Genève, basée en Suisse.\n",
    "        Utilisez les éléments de contexte et l'historique du chat suivants pour répondre aux questions. \n",
    "        Votre réponse doit être liée à l'Université de Genève uniquement. Si la question ne figure pas dans le contexte ou l'historique du chat, répondez \"Je suis désolé, je ne connais pas la réponse\".\n",
    "        Les réponses doivent être détaillées mais concises et courtes.\n",
    "        Respirez profondément et travaillez étape par étape.\n",
    "\n",
    "        Historique: {chat_history}\n",
    "        \n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "        Answer: \"\"\"\n",
    "\n",
    "\n",
    "    prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "\n",
    "    # init conversation memory\n",
    "    conversational_memory = ConversationBufferWindowMemory(\n",
    "        memory_key='chat_history',\n",
    "        input_key=\"question\",\n",
    "        k=3,\n",
    "        return_messages=True\n",
    "    )\n",
    "\n",
    "    # init retrievalQA chain\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            chain_type_kwargs={\"prompt\": prompt,\n",
    "                               \"memory\": conversational_memory\n",
    "                              },\n",
    "            return_source_documents=True,\n",
    "            verbose=False\n",
    "            )\n",
    "    \n",
    "    return qa\n",
    "\n",
    "def run_query(qa, query, session_id = None):\n",
    "\n",
    "    if session_id is None:\n",
    "        session_id = uuid.uuid4()\n",
    "        current_chat_history = ConversationBufferWindowMemory(\n",
    "        memory_key='chat_history',\n",
    "        input_key=\"question\",\n",
    "        k=3,\n",
    "        return_messages=True\n",
    "    )\n",
    "        chat_history_map[session_id] = current_chat_history\n",
    "    else:\n",
    "        current_chat_history = chat_history_map[session_id]\n",
    "    \n",
    "    res = qa({\"query\": query, \n",
    "              \"chat_history\": current_chat_history})\n",
    "    chat_history_map[session_id] = qa.combine_documents_chain.memory\n",
    "\n",
    "    return {\n",
    "        \"answer\": res[\"result\"],\n",
    "        \"source_documents\": res[\"source_documents\"],\n",
    "        \"session_id\" : session_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212353e2-6f51-44bf-bec9-20acf0ab743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = init_retrievalqa_chain()\n",
    "\n",
    "session_id = uuid.uuid4()\n",
    "\n",
    "current_chat_history = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    input_key=\"question\",\n",
    "    k=3,\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c329e0b-be26-4924-862a-1746e5044554",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa({\"query\": \"Quelles sont les conditions pour l'exmatriculation d'un étudiant ou d'une étudiante ?\", \n",
    "    \"chat_history\": current_chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c6875-98de-4b70-ae3e-e47adb81ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm.stream(\"Quelles sont les conditions pour l'exmatriculation d'un étudiant ou d'une étudiante ?\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_unigpt",
   "language": "python",
   "name": "venv_unigpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
